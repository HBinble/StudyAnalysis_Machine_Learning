{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 트리의 앙상블"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정형데이터에 가장 뛰어난 성능을 보이는 모델들 입니다.\n",
    "# 앙상블 모델들은 결정트리(Decision Tree)를 기반으로 만들어졌습니다.\n",
    "# 앙상블 모델들\n",
    "#   - 랜덤포레스트(Random Forest)\n",
    "#   - 엑스트라 트리(Extra Trees)\n",
    "#   - 그레디언트 부스팅(Gradient Boosting)\n",
    "#   - 히스토그램 기반 그레디언트 부스팅(Histogram-base Gradient Boosting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 랜덤포레스트(Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 앙상블 모델 중에 가장 대표격 모델로 사용됨\n",
    "# - 안정적인 성능으로 널리 사용됨\n",
    "# - 앙상블 모델 중에 가장 먼저 시도하는 모델이라고 보면 됩니다.\n",
    "# - 훈련데이터에서 과대적합되는 것을 막아줍니다.\n",
    "# - 검증데이터와 테스트데이터에서 안정적인 성능을 얻을 수 있음\n",
    "\n",
    "### 학습 개념\n",
    "# - 결정트리 하나하나를 랜덤하게 만들어서 숲을 만든다고 보시면 됩니다.\n",
    "# - 훈련데이터에서 랜덤하게 샘플을 추출하여 훈련을 완료한 후 \n",
    "#   다시 원본 훈련데이터에 반환을 합니다.\n",
    "# - 랜덤하게 추출 시 이전에 사용된 샘플을 사용할 수도 있음\n",
    "#   (중복을 허용함)\n",
    "\n",
    "### 부트스트랩 샘플(Bootstrap Sample)\n",
    "# - 위에 설명한 랜덤한 샘플 추출 시 중복을 허용하여 데이터를 샘플링 하는 방식\n",
    "# - 샘플 추출 방식\n",
    "#   1. 원본에서 랜덤 샘플 추출\n",
    "#   2. 훈련 이후 사용이 끝나면 원본에 반환\n",
    "#   3. 다시 원본에서 샘플 추출, 이때 중복 값 추출 될 수도 있음\n",
    "#   위 순서를 반복하면서 샘플링을 통해 훈련하는 방식을 랜덤포레스트가 적용하고 있음\n",
    "\n",
    "### *** 랜덤포레스트는 교차검증을 허용합니다. ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4872, 3) (4872,)\n",
      "(1625, 3) (1625,)\n"
     ]
    }
   ],
   "source": [
    "### 08_wine.csv 읽어들이고, 훈련 및 테스트 데이터까지 생성해주세요\n",
    "\n",
    "wine = pd.read_csv(\"./data/08_wine.csv\")\n",
    "wine.head()\n",
    "\n",
    "data = wine[[\"alcohol\", \"sugar\", \"pH\"]].to_numpy()\n",
    "target = wine[\"class\"].to_numpy()\n",
    "\n",
    "train_input, test_input, train_target, test_target = \\\n",
    "    train_test_split(data, target, random_state=42)\n",
    "    \n",
    "print(train_input.shape, train_target.shape)\n",
    "print(test_input.shape, test_target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 훈련모델 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련평가결과 = 0.997844759088341\n",
      "검증결과 = 0.8914208392565683\n"
     ]
    }
   ],
   "source": [
    "# 랜덤포레스트 패키지 : sklearn.ensemble\n",
    "# 랜덤포레스트 클래스(모델) : RandomForestClassfir\n",
    "# 교차검증 패키지 : sklearn.model_selection\n",
    "# 교차검증 : cross_validate()\n",
    "# 교차검증 후 훈련검증결과와 테스트검증결과 확인하기\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# 랜덤포레스트 객체생성 : 코어 모두 사용\n",
    "rf = RandomForestClassifier(n_jobs = -1, random_state=42)\n",
    "\n",
    "# 교차검증 진행\n",
    "# - return_train_score : 검증결과 반환받기\n",
    "scores = cross_validate(rf, train_input, train_target,\n",
    "                        return_train_score = True, n_jobs = -1)\n",
    "\n",
    "# 최종 훈련평가결과 및 검증결과\n",
    "scores\n",
    "\n",
    "print(\"훈련평가결과 =\", np.mean(scores[\"train_score\"]))\n",
    "print(\"검증결과 =\", np.mean(scores[\"test_score\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.23155241 0.49706658 0.27138101]\n"
     ]
    }
   ],
   "source": [
    "rf.fit(train_input, train_target)\n",
    "\n",
    "# 특성 중요도 조회하기(feature_importances_)\n",
    "# 랜덤포레스트 특징 : 여러가지 독립변수를 골고루 사용하기에 특성 중요도가 골고루 분포된다.\n",
    "print(rf.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 = 0.8981937602627258\n"
     ]
    }
   ],
   "source": [
    "### oob(out of back) 기능 사용\n",
    "# 훈련에 참여하지 못한 잔여 샘플 사용하는 기능\n",
    "# 기본은 사용 안함\n",
    "# 과대적합을 줄이고, 성능을 높임\n",
    "\n",
    "rf = RandomForestClassifier(oob_score = True, n_jobs = -1, random_state = 42)\n",
    "rf.fit(train_input, train_target)\n",
    "print(\"훈련 =\", rf.oob_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.엑스트라 트리(Extra Tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - 랜덤포레스트와 유사하게 작동\n",
    "# - 기본적으로 100개의 결정트리를 훈련함\n",
    "# - 랜덤포레스트와의 차이점\n",
    "#   : 부트스트랩 샘플링을 지원하지 않음\n",
    "#   : 훈련데이터 전체를 이용하여 결정트리를 생성\n",
    "#   : 무작위로 트리를 분리함\n",
    "# - 사용되는 속성 : splitter = \"random\" 무작위속성\n",
    "# - 장점\n",
    "#   : 과대적합을 막고, 검증데이터의 평가 값을 높일 수 있음\n",
    "#   : 특성 데이터가 많지 않은 경우에는 랜덤포레스트와 큰 차이가 없음\n",
    "# - 랜덤포레스트는 불순도 등 여러가지 조건에 따라 결정트리를 생성하기 때문에\n",
    "#   속도가 느린 반면, \n",
    "# - 엑스트라트리는 랜덤하게 결정트리를 생성하기에 속도가 다소 빠르다는 장점있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련평가결과 = 0.997844759088341\n",
      "검증결과 = 0.8903937240035804\n"
     ]
    }
   ],
   "source": [
    "### 사용패키지 : 랜덤포레스트와 동일\n",
    "# 사용되는 클래스(모델) : ExtraTreeClassifier\n",
    "\n",
    "### 코어 전체사용, train 및 test 결과값 출력\n",
    "### 교차검증 결과인 최종 train 및 test 결과 확인해주세요\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "et = ExtraTreesClassifier(n_jobs = -1, random_state = 42)\n",
    "scores = cross_validate(et, train_input, train_target, \n",
    "                        return_train_score = True, n_jobs = -1)\n",
    "\n",
    "# 최종 훈련평가 결과 및 검증결과\n",
    "scores\n",
    "\n",
    "print(\"훈련평가결과 =\", np.mean(scores[\"train_score\"]))\n",
    "print(\"검증결과 =\", np.mean(scores[\"test_score\"]))\n",
    "\n",
    "# 랜덤포레스트 결과\n",
    "# 훈련평가결과 = 0.997844759088341\n",
    "# 검증결과 = 0.8914208392565683"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.20702369 0.51313261 0.2798437 ]\n"
     ]
    }
   ],
   "source": [
    "et.fit(train_input, train_target)\n",
    "\n",
    "print(et.feature_importances_)\n",
    "\n",
    "# 랜덤포레스트 중요도 결과\n",
    "# [0.23155241 0.49706658 0.27138101]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 그레디언트 부스팅(Gradient Boosting) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 깊이(max_depth)가 얕은 결정트리를 사용함\n",
    "#   - 기본적으로 max_depth=3을 사용\n",
    "#   - 결정트리는 100개 사용\n",
    "### *** 기존에 다른 훈련모델 결과가 좋지 않을 때 사용하는 모델 ***\n",
    "# 기존 훈련모델의 오차를 많이 보완해 줍니다.\n",
    "# 성능 향상을 위한 모델로 주로 사용됩니다.\n",
    "# 과대적합에 강하며, 일반화(과대/과소적합이 없는 상태)에 강합니다.\n",
    "\n",
    "# 성능향상 테스트 방법\n",
    "#   - 결정트리의 갯수를 조절하면서 테스트 진행\n",
    "#   - 학습률을 지원하기 때문에 학습률의 값을 증가시키면서 테스트 진행\n",
    "#   : 기본 학습률은 0.1\n",
    "\n",
    "# 단점\n",
    "#   - 순서대로 트리를 추가(랜덤하지 않음)하지 않기 때문에\n",
    "#   - 훈련 속도는 느림\n",
    "#   - 이런 느린 속도를 개선한 모델이\n",
    "#   - \"히스토그램 기반 그레디언트 부스팅\" 모델임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그레디언트 부스팅 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련평가결과 = 0.8894704231708938\n",
      "검증결과 = 0.8715107671247301\n"
     ]
    }
   ],
   "source": [
    "# 사용되는 클래스(모델) : GradientBoostingClassifier\n",
    "# 객체 생성시 아무것도 안주고 seed값만 줍니다.\n",
    "# 교차검증시에는 train, test 결과값 출력합니다.\n",
    "\n",
    "### 코어 전체사용, train 및 test 결과값 출력\n",
    "### 교차검증 결과인 최종 train 및 test 결과 확인해주세요\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "scores = cross_validate(gb, train_input, train_target, \n",
    "                        return_train_score = True, n_jobs = -1)\n",
    "\n",
    "# 최종 훈련평가 결과 및 검증결과\n",
    "scores\n",
    "\n",
    "print(\"훈련평가결과 =\", np.mean(scores[\"train_score\"]))\n",
    "print(\"검증결과 =\", np.mean(scores[\"test_score\"]))\n",
    "\n",
    "# 랜덤포레스트 \n",
    "# 훈련평가결과 = 0.997844759088341\n",
    "# 검증결과 = 0.8914208392565683\n",
    "\n",
    "# 엑스트라 트리\n",
    "# 훈련평가결과 = 0.997844759088341\n",
    "# 검증결과 = 0.8903937240035804\n",
    "\n",
    "# 그레디언트 부스팅\n",
    "# 훈련평가결과 = 0.8894704231708938\n",
    "# 검증결과 = 0.8715107671247301"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.12517641 0.73300095 0.14182264]\n"
     ]
    }
   ],
   "source": [
    "gb.fit(train_input, train_target)\n",
    "\n",
    "print(gb.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트결과 = 0.8578461538461538\n"
     ]
    }
   ],
   "source": [
    "print(\"테스트결과 =\",gb.score(test_input, test_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습률 적용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련평가결과 = 0.8894704231708938\n",
      "검증결과 = 0.8715107671247301\n"
     ]
    }
   ],
   "source": [
    "# 학습률이 커지면 트리 보정을 강하게 하기 때문에,\n",
    "# 복잡한 모델을 만들어서 일반화 성능을 떨어뜨리게 된다.\n",
    "# - 학습률 : learning_rate = 0.1 기본값\n",
    "\n",
    "# 사용되는 클래스(모델) : GradientBoostingClassifier\n",
    "# 객체 생성시 아무것도 안주고 seed값만 줍니다.\n",
    "# 교차검증시에는 train, test 결과값 출력합니다.\n",
    "\n",
    "### 코어 전체사용, train 및 test 결과값 출력\n",
    "### 교차검증 결과인 최종 train 및 test 결과 확인해주세요\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb = GradientBoostingClassifier(n_estimators=100,\n",
    "                                learning_rate=0.1,\n",
    "                                random_state=42)\n",
    "\n",
    "scores = cross_validate(gb, train_input, train_target, \n",
    "                        return_train_score = True, n_jobs = -1)\n",
    "\n",
    "# 최종 훈련평가 결과 및 검증결과\n",
    "scores\n",
    "\n",
    "print(\"훈련평가결과 =\", np.mean(scores[\"train_score\"]))\n",
    "print(\"검증결과 =\", np.mean(scores[\"test_score\"]))\n",
    "\n",
    "# 랜덤포레스트 \n",
    "# 훈련평가결과 = 0.997844759088341 / 검증결과 = 0.8914208392565683\n",
    "\n",
    "# 엑스트라 트리\n",
    "# 훈련평가결과 = 0.997844759088341 / 검증결과 = 0.8903937240035804\n",
    "\n",
    "# 그레디언트 부스팅\n",
    "# 훈련평가결과 = 0.8894704231708938 / 검증결과 = 0.8715107671247301"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 히스토그램 기반 그레디언트 부스팅\n",
    "## (Histogram-base Gradient Boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련평가결과 = 0.9380129799494501\n",
      "검증결과 = 0.8805410414363187\n"
     ]
    }
   ],
   "source": [
    "### 사용하는 클래스(모델) : HistGradientBoostingClassifier\n",
    "\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "hgb = HistGradientBoostingClassifier(random_state=42)\n",
    "\n",
    "scores = cross_validate(hgb, train_input, train_target,\n",
    "                        return_train_score=True, n_jobs=-1)\n",
    "\n",
    "# 최종 훈련평가 결과 및 검증결과\n",
    "scores\n",
    "\n",
    "print(\"훈련평가결과 =\", np.mean(scores[\"train_score\"]))\n",
    "print(\"검증결과 =\", np.mean(scores[\"test_score\"]))\n",
    "\n",
    "# 랜덤포레스트 \n",
    "# 훈련평가결과 = 0.997844759088341 / 검증결과 = 0.8914208392565683\n",
    "\n",
    "# 엑스트라 트리\n",
    "# 훈련평가결과 = 0.997844759088341 / 검증결과 = 0.8903937240035804\n",
    "\n",
    "# 그레디언트 부스팅\n",
    "# 훈련평가결과 = 0.8894704231708938 / 검증결과 = 0.8715107671247301\n",
    "\n",
    "# 히스토그램기반 그레디언트 부스팅\n",
    "# 훈련평가결과 = 0.9380129799494501 / 검증결과 = 0.8805410414363187"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HistGradientBoostingClassifier(random_state=42)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hgb.fit(train_input, train_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트결과 = 0.8584615384615385\n"
     ]
    }
   ],
   "source": [
    "print(\"테스트결과 =\", hgb.score(test_input, test_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사이킷런 이외 다른 패키지에서 지원하는 \n",
    "## 히스토그램 기반 그레디언트 부스팅 기능 모델들"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. xgboost(Extreme Gradient Boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련평가결과 = 0.9614122399872658\n",
      "검증결과 = 0.8834151529510873\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier(tree_method=\"hist\", random_state=42)\n",
    "\n",
    "scores = cross_validate(xgb, train_input, train_target,\n",
    "                        return_train_score=True, n_jobs=-1)\n",
    "\n",
    "# 최종 훈련평가 결과 및 검증결과\n",
    "scores\n",
    "\n",
    "print(\"훈련평가결과 =\", np.mean(scores[\"train_score\"]))\n",
    "print(\"검증결과 =\", np.mean(scores[\"test_score\"]))\n",
    "\n",
    "# 랜덤포레스트 \n",
    "# 훈련평가결과 = 0.997844759088341 / 검증결과 = 0.8914208392565683\n",
    "\n",
    "# 엑스트라 트리\n",
    "# 훈련평가결과 = 0.997844759088341 / 검증결과 = 0.8903937240035804\n",
    "\n",
    "# 그레디언트 부스팅\n",
    "# 훈련평가결과 = 0.8894704231708938 / 검증결과 = 0.8715107671247301\n",
    "\n",
    "# 히스토그램기반 그레디언트 부스팅\n",
    "# 훈련평가결과 = 0.9380129799494501 / 검증결과 = 0.8805410414363187\n",
    "\n",
    "# XGB(Extreme Gradient Boosting)\n",
    "# 훈련평가결과 = 0.9614122399872658 / 검증결과 = 0.8834151529510873"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - 마이크로소프트에서 만든 히스토그램 기반 그레디언트 부스트 패키지\n",
    "# - 훈련 속도가 매우 빠름\n",
    "# - 최신 기술을 많이 적용하고 있어서, 인기가 올라가고 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lightgbm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\admin\\STUDY\\StudyPython_Machine_Learning\\Machine_Learning\\6day\\02_트리의_앙상블모델들_RF_ET_GB_HGB.ipynb 셀 32\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/admin/STUDY/StudyPython_Machine_Learning/Machine_Learning/6day/02_%ED%8A%B8%EB%A6%AC%EC%9D%98_%EC%95%99%EC%83%81%EB%B8%94%EB%AA%A8%EB%8D%B8%EB%93%A4_RF_ET_GB_HGB.ipynb#X42sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# 아나콘다 사용 : conda install -c conda-forge lightgbm\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/admin/STUDY/StudyPython_Machine_Learning/Machine_Learning/6day/02_%ED%8A%B8%EB%A6%AC%EC%9D%98_%EC%95%99%EC%83%81%EB%B8%94%EB%AA%A8%EB%8D%B8%EB%93%A4_RF_ET_GB_HGB.ipynb#X42sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# 파이썬 기반 사용 : pip install lightgbm \u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/admin/STUDY/StudyPython_Machine_Learning/Machine_Learning/6day/02_%ED%8A%B8%EB%A6%AC%EC%9D%98_%EC%95%99%EC%83%81%EB%B8%94%EB%AA%A8%EB%8D%B8%EB%93%A4_RF_ET_GB_HGB.ipynb#X42sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# 가상환경에서 설치\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/admin/STUDY/StudyPython_Machine_Learning/Machine_Learning/6day/02_%ED%8A%B8%EB%A6%AC%EC%9D%98_%EC%95%99%EC%83%81%EB%B8%94%EB%AA%A8%EB%8D%B8%EB%93%A4_RF_ET_GB_HGB.ipynb#X42sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlightgbm\u001b[39;00m \u001b[39mimport\u001b[39;00m LGBMClassifier\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/admin/STUDY/StudyPython_Machine_Learning/Machine_Learning/6day/02_%ED%8A%B8%EB%A6%AC%EC%9D%98_%EC%95%99%EC%83%81%EB%B8%94%EB%AA%A8%EB%8D%B8%EB%93%A4_RF_ET_GB_HGB.ipynb#X42sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m lgb \u001b[39m=\u001b[39m LGBMClassifier(random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/admin/STUDY/StudyPython_Machine_Learning/Machine_Learning/6day/02_%ED%8A%B8%EB%A6%AC%EC%9D%98_%EC%95%99%EC%83%81%EB%B8%94%EB%AA%A8%EB%8D%B8%EB%93%A4_RF_ET_GB_HGB.ipynb#X42sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m scores \u001b[39m=\u001b[39m cross_validate(lgb, train_input, train_target,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/admin/STUDY/StudyPython_Machine_Learning/Machine_Learning/6day/02_%ED%8A%B8%EB%A6%AC%EC%9D%98_%EC%95%99%EC%83%81%EB%B8%94%EB%AA%A8%EB%8D%B8%EB%93%A4_RF_ET_GB_HGB.ipynb#X42sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m                         return_train_score\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, n_jobs\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'lightgbm'"
     ]
    }
   ],
   "source": [
    "# 아나콘다 사용 : conda install -c conda-forge lightgbm\n",
    "# 파이썬 기반 사용 : pip install lightgbm \n",
    "# 가상환경에서 설치\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "lgb = LGBMClassifier(random_state=42)\n",
    "\n",
    "scores = cross_validate(lgb, train_input, train_target,\n",
    "                        return_train_score=True, n_jobs=-1)\n",
    "\n",
    "# 최종 훈련평가 결과 및 검증결과\n",
    "scores\n",
    "\n",
    "print(\"훈련평가결과 =\", np.mean(scores[\"train_score\"]))\n",
    "print(\"검증결과 =\", np.mean(scores[\"test_score\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_kernel",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
